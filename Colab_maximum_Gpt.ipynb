{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sportcman/Gpt/blob/main/Colab_maximum_Gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShyeFEzoVubz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeHKTntzb5ZA"
      },
      "outputs": [],
      "source": [
        "pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESkXoUTqb_V4"
      },
      "outputs": [],
      "source": [
        "pip install torch_xla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-P9pYBOcLOE"
      },
      "outputs": [],
      "source": [
        "pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isNlobqPcNRm"
      },
      "outputs": [],
      "source": [
        "pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRp5a2KqO3kD"
      },
      "outputs": [],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urx8NhvpcW_w"
      },
      "source": [
        "**`Создание модели. Оптимальна.`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE56mRrRu8qf"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/gdrive/MyDrive/TrenerGpt/tokenizer')\n",
        "\n",
        "model_config = GPT2Config(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    n_layer=22,\n",
        "    n_head=32,\n",
        "    n_embd=2048,\n",
        "    intermediate_size=3072,\n",
        "    hidden_size=1536,\n",
        "    max_position_embeddings=3072,\n",
        "    gradient_checkpointing=True,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    sep_token_id=tokenizer.sep_token_id,\n",
        "    use_cache=True,\n",
        "    layer_norm_epsilon=1e-5,\n",
        "    initializer_range=0.02,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        "    tie_word_embeddings=True\n",
        ")\n",
        "model = GPT2LMHeadModel(config=model_config)\n",
        "model.set_input_embeddings(model.resize_token_embeddings(len(tokenizer)))\n",
        "\n",
        "model.save_pretrained('/gdrive/MyDrive/TrenerGpt/model')\n",
        "tokenizer.save_pretrained('/gdrive/MyDrive/TrenerGpt/model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opUVidQ7H674"
      },
      "source": [
        "Обучение МОДЕЛИ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUTXezYpyxKy"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, block_size):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "\n",
        "        self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
        "\n",
        "def train(model, dataset, tokenizer, device, batch_size, epochs, model_path):\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=0.95)\n",
        "\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_progress = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in epoch_progress:\n",
        "            batch = batch.to(device)\n",
        "            inputs, labels = batch, batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_progress.set_postfix(loss=loss.item())\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
        "\n",
        "    model.save_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--num_epochs', type=int, default=1) # коллличество эпох\n",
        "    parser.add_argument('--dataset_path', type=str, default=\"/gdrive/MyDrive/TrenerGpt/dataset.txt\")\n",
        "    parser.add_argument('--device', type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    args = parser.parse_known_args()[0]\n",
        "\n",
        "    dataset_path = args.dataset_path\n",
        "    model_path = \"/gdrive/MyDrive/TrenerGpt/model\"\n",
        "    num_epochs = args.num_epochs\n",
        "    batch_size = 2 #  должен учитывать количество ядер процессора и доступную оперативную память\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    try:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from {model_path}, loading GPT-2 base model instead. Error: {e}\")\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    dataset = CustomTextDataset(tokenizer, dataset_path, block_size=64) #  Учитывает среднюю длину текстовых фрагментов, которые модель будет обрабатывать.\n",
        "\n",
        "    train(model, dataset, tokenizer, device, batch_size, num_epochs, model_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFRYzY4MhJlR"
      },
      "source": [
        "***Скрипт для проверки ответов.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOaVcWr1NXdX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "class GPT2Generator:\n",
        "    def __init__(self, model_path):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path).to(self.device)\n",
        "\n",
        "    def generate_text(self, input_text, temperature_value, length_value, num_results, no_repeat_ngram_size):\n",
        "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt').to(self.device)\n",
        "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=length_value,\n",
        "            num_return_sequences=num_results,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            repetition_penalty=1.5,\n",
        "            temperature=temperature_value,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        result_text = \"\"\n",
        "        for i, output in enumerate(outputs):\n",
        "            generated_text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
        "            result_text += f\"Результат {i+1}:\\n{generated_text}\\n\\n\"\n",
        "\n",
        "        return result_text\n",
        "\n",
        "gpt2_generator = GPT2Generator(\"/gdrive/MyDrive/TrenerGpt/model\")\n",
        "temperature_value = 0.1\n",
        "length_value = 100\n",
        "num_results = 1\n",
        "ngram_value = 2\n",
        "\n",
        "def generate_text():\n",
        "    input_text = input(\"Введи затравку: \")\n",
        "    result_text = gpt2_generator.generate_text(input_text, temperature_value, length_value, num_results, ngram_value)\n",
        "    print(result_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        user_input = input(\"Выберите действие (1 - сгенерировать текст, 2 - выход): \")\n",
        "        if user_input == \"1\":\n",
        "            generate_text()\n",
        "        elif user_input == \"2\":\n",
        "            break\n",
        "        else:\n",
        "            print(\"Некорректный ввод. Попробуйте снова.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDY6RjsUocW0V0O8tfxUHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}